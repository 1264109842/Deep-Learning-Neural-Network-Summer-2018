{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "# Enable Eager Execution\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"Tensorflow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "X=mnist.train.images\n",
    "Y=mnist.train.labels[:,0].reshape(55000, 1)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example features:  tf.Tensor(\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ], shape=(784,), dtype=float32)\n",
      "example label:  tf.Tensor([0.], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensorflow tensors\n",
    "X = tf.convert_to_tensor(X)\n",
    "Y = tf.convert_to_tensor(Y)\n",
    "# Contruct tensorflow dataset object from tensors\n",
    "tfds = tf.data.Dataset.from_tensors((X,Y))\n",
    "# View a single example entry from a batch\n",
    "features, label = iter(tfds).next()\n",
    "print(\"example features: \", features[0])\n",
    "print(\"example label: \", label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Logistic Regression Model (Sigmoid)\n",
    "sig_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", input_shape=(784,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward propogation\n",
    "# loss function to be binary crossentropy\n",
    "def forward_prop(model, x, y):\n",
    "    a = model(x)\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels=y,logits=a)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backward propogation\n",
    "# Object Gradient Tape does the magic of calculating gradients\n",
    "#tape = tf.GradientTape()\n",
    "def backward_prop(model, tape, loss):\n",
    "    grad = tape.gradient(loss, model.variables)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "lr = 1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6979889273643494\n",
      "Loss: 0.6975814700126648\n",
      "Loss: 0.6972430348396301\n",
      "Loss: 0.6969552636146545\n",
      "Loss: 0.6967069506645203\n",
      "Loss: 0.6964920163154602\n",
      "Loss: 0.6963039636611938\n",
      "Loss: 0.6961367726325989\n",
      "Loss: 0.6959872245788574\n",
      "Loss: 0.6958532929420471\n",
      "Loss: 0.6957332491874695\n",
      "Loss: 0.695622444152832\n",
      "Loss: 0.6955223083496094\n",
      "Loss: 0.6954305171966553\n",
      "Loss: 0.6953459978103638\n",
      "Loss: 0.6952683329582214\n",
      "Loss: 0.6951958537101746\n",
      "Loss: 0.6951286792755127\n",
      "Loss: 0.6950662136077881\n",
      "Loss: 0.6950088143348694\n",
      "Loss: 0.6949539184570312\n",
      "Loss: 0.6949029564857483\n",
      "Loss: 0.6948545575141907\n",
      "Loss: 0.6948087215423584\n",
      "Loss: 0.6947664618492126\n",
      "Loss: 0.6947264671325684\n",
      "Loss: 0.6946877837181091\n",
      "Loss: 0.6946513056755066\n",
      "Loss: 0.6946169137954712\n",
      "Loss: 0.6945834755897522\n",
      "Loss: 0.6945523619651794\n",
      "Loss: 0.69452303647995\n",
      "Loss: 0.6944941878318787\n",
      "Loss: 0.6944671273231506\n",
      "Loss: 0.6944410800933838\n",
      "Loss: 0.6944153904914856\n",
      "Loss: 0.6943914890289307\n",
      "Loss: 0.6943686604499817\n",
      "Loss: 0.6943466067314148\n",
      "Loss: 0.6943265795707703\n",
      "Loss: 0.6943063139915466\n",
      "Loss: 0.6942867636680603\n",
      "Loss: 0.6942683458328247\n",
      "Loss: 0.6942508816719055\n",
      "Loss: 0.6942333579063416\n",
      "Loss: 0.6942166686058044\n",
      "Loss: 0.6942001581192017\n",
      "Loss: 0.6941845417022705\n",
      "Loss: 0.6941690444946289\n",
      "Loss: 0.6941539645195007\n",
      "Loss: 0.6941394209861755\n",
      "Loss: 0.6941254138946533\n",
      "Loss: 0.6941128373146057\n",
      "Loss: 0.6940998435020447\n",
      "Loss: 0.6940875053405762\n",
      "Loss: 0.6940750479698181\n",
      "Loss: 0.6940629482269287\n",
      "Loss: 0.6940518021583557\n",
      "Loss: 0.694040060043335\n",
      "Loss: 0.6940296292304993\n",
      "Loss: 0.6940190196037292\n",
      "Loss: 0.6940086483955383\n",
      "Loss: 0.6939989328384399\n",
      "Loss: 0.6939895749092102\n",
      "Loss: 0.6939799785614014\n",
      "Loss: 0.6939706206321716\n",
      "Loss: 0.6939612030982971\n",
      "Loss: 0.6939525008201599\n",
      "Loss: 0.69394451379776\n",
      "Loss: 0.6939362287521362\n",
      "Loss: 0.6939278244972229\n",
      "Loss: 0.693919837474823\n",
      "Loss: 0.6939122080802917\n",
      "Loss: 0.6939043998718262\n",
      "Loss: 0.693897008895874\n",
      "Loss: 0.6938897967338562\n",
      "Loss: 0.6938826441764832\n",
      "Loss: 0.6938759684562683\n",
      "Loss: 0.69386887550354\n",
      "Loss: 0.6938621401786804\n",
      "Loss: 0.6938555836677551\n",
      "Loss: 0.693849503993988\n",
      "Loss: 0.6938431859016418\n",
      "Loss: 0.69383704662323\n",
      "Loss: 0.6938312649726868\n",
      "Loss: 0.6938254833221436\n",
      "Loss: 0.6938197016716003\n",
      "Loss: 0.6938141584396362\n",
      "Loss: 0.6938090920448303\n",
      "Loss: 0.6938036680221558\n",
      "Loss: 0.6937980055809021\n",
      "Loss: 0.6937928795814514\n",
      "Loss: 0.6937880516052246\n",
      "Loss: 0.6937829256057739\n",
      "Loss: 0.6937776803970337\n",
      "Loss: 0.6937727332115173\n",
      "Loss: 0.6937677264213562\n",
      "Loss: 0.6937631964683533\n",
      "Loss: 0.6937583684921265\n",
      "Loss: 0.6937538385391235\n"
     ]
    }
   ],
   "source": [
    "# The Big For-Loop Training\n",
    "m = 55000\n",
    "dc = tf.constant(1/m)\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_avg = tfe.metrics.Mean()\n",
    "    batch_grad = None\n",
    "    \n",
    "    for x,y in tfds:\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss = forward_prop(sig_model, x, y)\n",
    "        grad = backward_prop(sig_model, tape, loss)\n",
    "        loss_avg(loss)\n",
    "        optimizer.apply_gradients(zip(grad, sig_model.variables))\n",
    "    \n",
    "    print(\"Loss: {}\".format(loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
